{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee27f44-5848-4bf9-8f13-ebd0191655dc",
   "metadata": {},
   "source": [
    "# 2. List of different approaches of how such game can be re-created. You can also use consider the other type of the data than we are providing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b310f-fa16-4e6b-9d70-4db66a824fbb",
   "metadata": {},
   "source": [
    "Multiple method could be used:\n",
    "\n",
    "1) data augmentation, noise injection to norm and repeat other param\n",
    "2) Use markov chain to model action, then generate size of list using normal distribution, and generate norm value using normal distribution\n",
    "3) Use markov chain to model action, then generate size of list using normal distribution, and generate norm value using GAN\n",
    "4) Use markov chain to model action, then generate size of list using normal distribution, and generate norm value using LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ad686-1b82-45a4-ba52-f69ebb7ddc56",
   "metadata": {},
   "source": [
    "# Please fully describe at least one approach you would choose (in jupyter notebook or some additional pdf): \n",
    "## a. The chosen architecture/algorithm. Why the decision was made, why it makes sense, and what kind of input it assumes. If the mathematical theory for the chosen approach is too complicated, the flow chart is enough.\n",
    "## b. The pre- / post-processing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701dd2ee-5e35-417a-a5e6-f583c6f88d5c",
   "metadata": {},
   "source": [
    "A) \n",
    "\n",
    "\n",
    "## Summary of Solution:\n",
    "\n",
    "I employed a multi-pronged approach to model and generate the desired sequences:\n",
    "\n",
    "### 1. **Markov Chain for Action Sequencing**:\n",
    "I utilized a Markov chain to model and generate action sequences. This approach ensures that the action labels are generated in a meaningful and coherent order, mimicking real-world scenarios.\n",
    "\n",
    "### 2. **Normal Distribution for List Size Determination**:\n",
    "Given the significance of maintaining the gait length, I chose to model the size of the list using a normal distribution. This approach ensures that the generated lengths closely resemble the real-world data distribution.\n",
    "\n",
    "### 3. **LSTM for Norm Value Generation**:\n",
    "For the generation of norm values, I deployed an LSTM (Long Short-Term Memory) model, which is adept at handling and generating sequences. The LSTM architecture comprised two layers:\n",
    "- A layer for norm values: This captures the sequence of previous values.\n",
    "- A layer for labels: Recognizing that the norm values are influenced by actions, this layer ensures that the generated norm values are consistent with the associated action labels.\n",
    "\n",
    "## Reflection:\n",
    "Upon the completion of this model, I believe that while the LSTM approach has its strengths, there might be areas of potential refinement. Some generated values appear unconventional, indicating the possibility of overfitting or over-complexity. We can delve deeper into these aspects and discuss potential improvements during our interview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eecc43-5942-4931-a3c5-9c81267cab2f",
   "metadata": {},
   "source": [
    "B)\n",
    "\n",
    "\n",
    "\n",
    "Proper preprocessing of the data was essential to prepare it for modeling. The following steps were undertaken:\n",
    "- **Encoding Labels**: The action labels were encoded to numerical values, making them suitable for model processing.\n",
    "- **Standardizing Norm Values**: To stabilize the LSTM and ensure consistent input, the norm values were standardized. This standardization helps in ensuring smoother training and better performance of the LSTM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2dee1-64d9-4365-a6da-d7eb0ee8381e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2e0a104-a654-4a81-963a-530a05fc5d14",
   "metadata": {},
   "source": [
    "# 5. Parametrize your fitted algorithm/program for recreating the game in the following way:\n",
    "\n",
    "Bonus: It will be possible to generate game with specific type of play - e.g\n",
    "more attacking game (there will be more passes, shots), defending game\n",
    "(more tackles, interceptions, etc.), or just normal game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370334e1-8476-44d7-9f1a-263e343b9b2e",
   "metadata": {},
   "source": [
    "##  1. Adjusting the Markov Chain for Specific Actions:\n",
    "To generate specific actions with greater or lesser frequency, the transition probabilities in the Markov chain can be fine-tuned. For instance:\n",
    "\n",
    "### For a Striker: \n",
    "After observing that the probability of a shot post a dribble is 0.09, one could reduce the probabilities of other subsequent actions post-dribble, transferring that proportion to the shooting probability. It's crucial to strike a balance to avoid excessive shooting actions, which may deviate from realistic play.\n",
    "\n",
    "### For a Defender: \n",
    "To simulate defensive actions like tackling with greater frequency, the probabilities of actions like tackling could be increased post certain preceding actions.\n",
    "\n",
    "## 2. Balancing the Coefficients:\n",
    "The key challenge lies in adjusting the coefficients in a manner that the generated sequences remain realistic. While emphasizing a specific action, care should be taken to ensure that the player doesn't overly rely on that singular action, as this would distort the authenticity of the gameplay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634f211-4537-420f-b801-d493569b19ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baed67e-971d-476e-bd98-8a6410de3043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>[23.177037336396975, 23.361525285249378, 21.53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>[20.998214383911275, 19.182798059840767, 18.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>[21.95259682019565, 20.62720484424047, 22.5554...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>[19.39209748358647, 19.30460665293087, 18.9787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rest</td>\n",
       "      <td>[22.069263037713093, 19.672270483203395, 19.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>walk</td>\n",
       "      <td>[23.337305769963503, 20.210987911153104, 25.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>walk</td>\n",
       "      <td>[22.470322813933603, 22.552427730975246, 23.84...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>walk</td>\n",
       "      <td>[43.833612705797144, 46.18045998580312, 37.492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>walk</td>\n",
       "      <td>[30.927599255773355, 31.26358258808756, 28.286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>walk</td>\n",
       "      <td>[30.94168559492879, 47.060036178960765, 37.195...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1187 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               norm\n",
       "0     walk  [23.177037336396975, 23.361525285249378, 21.53...\n",
       "1     walk  [20.998214383911275, 19.182798059840767, 18.27...\n",
       "2     walk  [21.95259682019565, 20.62720484424047, 22.5554...\n",
       "3     walk  [19.39209748358647, 19.30460665293087, 18.9787...\n",
       "4     rest  [22.069263037713093, 19.672270483203395, 19.88...\n",
       "...    ...                                                ...\n",
       "1182  walk  [23.337305769963503, 20.210987911153104, 25.06...\n",
       "1183  walk  [22.470322813933603, 22.552427730975246, 23.84...\n",
       "1184  walk  [43.833612705797144, 46.18045998580312, 37.492...\n",
       "1185  walk  [30.927599255773355, 31.26358258808756, 28.286...\n",
       "1186  walk  [30.94168559492879, 47.060036178960765, 37.195...\n",
       "\n",
       "[1187 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Concatenate, Input, Flatten, Reshape, RepeatVector, Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import re\n",
    "from keras.models import load_model\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the file path. Replace with the actual path to the file if different.\n",
    "file_path = 'data/match_1.json'\n",
    "# Load the JSON file into a pandas DataFrame with records orientation.\n",
    "match_1_data = pd.read_json(file_path, orient='records')\n",
    "\n",
    "file_path = 'data/match_2.json'\n",
    "match_2_data = pd.read_json(file_path, orient='records')\n",
    "\n",
    "\n",
    "# Combiner les données des deux matches\n",
    "data = pd.concat([match_1_data, match_2_data], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9fb7a8-3dcc-432a-86cb-b5e28541f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_frequencies(data):\n",
    "    \"\"\"\n",
    "    Compute the frequency of transitions between different actions in the data.\n",
    "    \n",
    "    Args:\n",
    "    - data (DataFrame): The dataset containing labeled sequences of actions.\n",
    "\n",
    "    Returns:\n",
    "    - matrix (dict): A dictionary representing the frequency of transitions \n",
    "                     between different actions.\n",
    "    \"\"\"\n",
    "    matrix = {}  # Initialize an empty dictionary to hold transition frequencies.\n",
    "    \n",
    "    # Loop through each row of the data except the last one\n",
    "    for i in range(len(data)-1):\n",
    "        # Get the current action from the 'label' column\n",
    "        current_action = data.iloc[i]['label']\n",
    "        # Get the next action from the 'label' column\n",
    "        next_action = data.iloc[i+1]['label']\n",
    "        \n",
    "        # Check if the current action is already in the matrix\n",
    "        if current_action not in matrix:\n",
    "            matrix[current_action] = {}  # If not, initialize it with an empty dictionary.\n",
    "        \n",
    "        # Check if the next action is already a key under the current action\n",
    "        if next_action not in matrix[current_action]:\n",
    "            matrix[current_action][next_action] = 1  # If not, initialize it with a count of 1.\n",
    "        else:\n",
    "            matrix[current_action][next_action] += 1  # If yes, increment the count.\n",
    "    \n",
    "    # Loop to normalize the transition counts to get transition probabilities\n",
    "    for current_action, transitions in matrix.items():\n",
    "        # Calculate the total count of transitions from the current action\n",
    "        total = sum(transitions.values())\n",
    "        # Normalize each count by the total\n",
    "        for next_action, count in transitions.items():\n",
    "            matrix[current_action][next_action] = count / total\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def generate_actions(transitions, n):\n",
    "    \"\"\"\n",
    "    Generate a sequence of actions based on the transition probabilities.\n",
    "\n",
    "    Args:\n",
    "    - transitions (dict): A dictionary representing the transition probabilities.\n",
    "    - n (int): Length of the sequence to be generated.\n",
    "\n",
    "    Returns:\n",
    "    - sequence (list): Generated sequence of actions.\n",
    "    \"\"\"\n",
    "    # Start with the \"walk\" action\n",
    "    current_action = \"walk\"\n",
    "    sequence = [current_action]\n",
    "\n",
    "    for _ in range(n-1):\n",
    "        # Check if there are possible transitions from the current action\n",
    "        if current_action in transitions and transitions[current_action]:\n",
    "            next_actions = list(transitions[current_action].keys())\n",
    "            next_probs = list(transitions[current_action].values())\n",
    "            current_action = np.random.choice(next_actions, p=next_probs)\n",
    "        else:\n",
    "            # If no possible transitions, choose a random action (excluding \"walk\" to avoid repetition)\n",
    "            current_action = np.random.choice([action for action in transitions.keys() if action != \"walk\"])\n",
    "        sequence.append(current_action)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5f3342-82eb-41af-98ed-f529f79b125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_specific_stats(data):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of lengths for each label.\n",
    "\n",
    "    Args:\n",
    "    - data (DataFrame): The dataset containing sequences and their labels.\n",
    "\n",
    "    Returns:\n",
    "    - stats (dict): A dictionary where keys are labels and values are \n",
    "                    (mean, std) tuples for that label's lengths.\n",
    "    \"\"\"\n",
    "    # Compute lengths for each row and then group by the label\n",
    "    lengths_by_label = data.groupby('label')['norm'].apply(lambda x: x.apply(len))\n",
    "    \n",
    "    means = lengths_by_label.groupby('label').mean()\n",
    "    stds = lengths_by_label.groupby('label').std()\n",
    "    \n",
    "    return {label: (mean, std) for label, mean, std in zip(means.index, means, stds)}\n",
    "\n",
    "def generate_length_for_action(action, stats):\n",
    "    \"\"\"Generate a sequence length for a given action based on stored statistics.\"\"\"\n",
    "    mean, std = stats[action]\n",
    "    return int(abs(np.random.normal(mean, std)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d5a475-ae91-4daf-91b9-49ab6b7e93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the build_transition_matrix function to get transition probabilities\n",
    "transitions = compute_transition_frequencies(data)\n",
    "\n",
    "stats = get_label_specific_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3265ed-f1a2-40de-bf21-ed7c0183d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the build_transition_matrix function to get transition probabilities\n",
    "# transitions = compute_transition_frequencies(data)\n",
    "# # Generate actions for 10 minutes\n",
    "# new_actions = generate_actions(transitions, 5)  # Adjust this based on your data\n",
    "\n",
    "\n",
    "# stats = get_label_specific_stats(data)\n",
    "# print(new_actions)\n",
    "# print(stats)\n",
    "# # Generate new sequences\n",
    "# new_sequences = []\n",
    "# for action in new_actions:\n",
    "\n",
    "#     length = generate_length_for_action(action, stats)\n",
    "#     print(length)\n",
    "#     new_sequences.append({\"label\": action, \"norm\": [0] * length})\n",
    "\n",
    "# print(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee5ccb-62a4-494e-a8fd-9d782d3db1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbde85e-1354-4949-9a0c-e17bb441e924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d628e33-d974-4c61-bd2a-9ed0695ccf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654e246-bed2-46aa-bd62-6d5b9a0c4aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af458bf-6e71-4950-8706-72e178e0847a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed998c-b299-4c98-a508-addccafd0e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de436c12-0615-4756-8497-7dd42e13dfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac43dd2-3efc-4229-ab49-3b320b12bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e71315-e11d-417f-a0e1-9ac3b059d39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d062c6-e2f5-4b82-a468-3ab3d6f5ba14",
   "metadata": {},
   "source": [
    "# Modeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197ea4b-15d8-41b9-a1ff-a14036c39071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99410e73-139c-4e68-909d-1bbc72cb9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(data['label']))\n",
    "\n",
    "# Define the input shapes\n",
    "sequence_length = 20\n",
    "\n",
    "# Sequence input\n",
    "sequence_input = Input(shape=(sequence_length, 1), name='sequence_input')\n",
    "\n",
    "# Label input and embedding\n",
    "label_input = Input(shape=(1,), name='label_input')\n",
    "label_embedding = Embedding(input_dim=num_labels, output_dim=8)(label_input)  # increased embedding size\n",
    "label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "# Expand dimensions and tile the embedding to match the sequence length\n",
    "label_embedding = RepeatVector(sequence_length)(label_embedding)\n",
    "\n",
    "# Concatenate sequence and label embedding\n",
    "merged_input = concatenate([sequence_input, label_embedding])\n",
    "\n",
    "# LSTM layers with dropout\n",
    "lstm_out = LSTM(32, return_sequences=True, dropout=0.0)(merged_input)  # adjusted LSTM units and dropout\n",
    "lstm_out = LSTM(16, dropout=0.0)(lstm_out)  # adjusted LSTM units and dropout\n",
    "\n",
    "# Dense layer to predict the next value in the sequence\n",
    "output = Dense(1, activation='linear')(lstm_out)\n",
    "\n",
    "# Compile the model with the default learning rate\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model = Model(inputs=[sequence_input, label_input], outputs=output)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043432ee-97d8-467b-9ff1-565ea57ba581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a99e14-f307-45d2-ac49-359baa48bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = data['norm'].tolist()\n",
    "\n",
    "# Compute the mean and standard deviation of your sequences\n",
    "all_values = [item for sublist in sequences for item in sublist]\n",
    "mean = np.mean(all_values)\n",
    "std = np.std(all_values)\n",
    "\n",
    "# Standardize the sequences\n",
    "sequences = [(np.array(seq) - mean) / std for seq in sequences]\n",
    "sequences = [seq.tolist() for seq in sequences]\n",
    "\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# Convert labels to unique integers for embedding\n",
    "label_to_int = {label: i for i, label in enumerate(set(labels))}\n",
    "encoded_labels = [label_to_int[label] for label in labels]\n",
    "\n",
    "def prepare_training_data(sequences, encoded_labels, sequence_length):\n",
    "    X_seq, X_label, Y = [], [], []\n",
    "    for seq, label in zip(sequences, encoded_labels):\n",
    "        for i in range(1, min(len(seq), sequence_length)):\n",
    "            padded_seq = seq[:i] + [0] * (sequence_length - i)\n",
    "            if len(padded_seq) != sequence_length:\n",
    "                print(f\"Unexpected sequence length. Before padding: {len(seq[:i])}, After padding: {len(padded_seq)}\")\n",
    "                continue\n",
    "            X_seq.append(padded_seq)\n",
    "            X_label.append(label)\n",
    "            Y.append(seq[i])\n",
    "    X_seq = np.array(X_seq)\n",
    "    X_label = np.array(X_label)\n",
    "    Y = np.array(Y)\n",
    "    return X_seq, X_label, Y\n",
    "\n",
    "X_seq, X_label, Y = prepare_training_data(sequences, encoded_labels, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "030629fa-eea2-4490-bddc-d1724efb0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictSamples(Callback):\n",
    "    def __init__(self, X_seq_sample, X_label_sample, Y_sample):\n",
    "        super(PredictSamples, self).__init__()\n",
    "        self.X_seq_sample = X_seq_sample\n",
    "        self.X_label_sample = X_label_sample\n",
    "        self.Y_sample = Y_sample\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        predictions = self.model.predict([self.X_seq_sample, self.X_label_sample])\n",
    "        print(\"\\nSample predictions after epoch {}:\".format(epoch+1))\n",
    "        for actual, predicted in zip(self.Y_sample, predictions):\n",
    "            print(\"Actual: {:.3f}, Predicted: {:.3f}\".format(actual, predicted[0]))\n",
    "        print(\"----------------------\")\n",
    "\n",
    "# Choose a small sample of your data\n",
    "sample_size = 10\n",
    "X_seq_sample = X_seq[:sample_size]\n",
    "X_label_sample = X_label[:sample_size]\n",
    "Y_sample = Y[:sample_size]\n",
    "\n",
    "# Use the custom callback during training\n",
    "sample_callback = PredictSamples(X_seq_sample, X_label_sample, Y_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec84c4fb-be39-4ef5-9dfa-366087192ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When training, include this callback\n",
    "# history = model.fit([X_seq, X_label], Y, epochs=15, batch_size=32, validation_split=0.2, callbacks=[sample_callback])\n",
    "\n",
    "\n",
    "# # Plotting the training and validation loss\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Training and Validation MSE')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('MSE Value')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c6d921-7058-4020-a048-0be1f92219b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my model:\n",
    "# model.save('model/lstm_norm_values.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c270b2-354d-4ed8-afce-cf43859c4d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4507f07-9b5e-4bae-8f64-57e7024c0e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655e173-012b-452e-a161-6d9e6a22158e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbf6f72-0dab-4155-9f00-f9fa6add1d7c",
   "metadata": {},
   "source": [
    "# Prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021918cc-3e40-4931-a521-0d6748a56f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_sequence(model, sequence_length, action):\n",
    "    # Initialize a zero sequence for warmup\n",
    "    current_sequence = np.zeros(sequence_length).reshape(1, sequence_length)\n",
    "    warmup_output = []\n",
    "    \n",
    "    # Get the encoded label for the action\n",
    "    encoded_label = get_encoded_label_for_action(action)\n",
    "    encoded_label_array = np.array([encoded_label]).reshape(1, -1)\n",
    "    \n",
    "    for _ in range(sequence_length):\n",
    "        next_value = model.predict([current_sequence, encoded_label_array])\n",
    "        warmup_output.append(next_value[0][0])\n",
    "        current_sequence = np.append(current_sequence[0, 1:], next_value[0]).reshape(1, sequence_length)\n",
    "    \n",
    "    return np.array(warmup_output)\n",
    "\n",
    "def fill_table_with_sequence(model, action_table_length, sequence_length, action, last_20_values):\n",
    "    # Create a zero-filled action table based on the provided length\n",
    "    action_table = np.zeros(action_table_length)\n",
    "    \n",
    "    # Check if the last_20_values are all zeros (first action scenario)\n",
    "    if np.all(last_20_values == 0):\n",
    "        warmup_output = warmup_sequence(model, sequence_length, action)\n",
    "        action_table[:min(len(warmup_output), action_table_length)] = warmup_output[:action_table_length]\n",
    "        current_sequence = warmup_output.reshape(1, sequence_length)\n",
    "        start_idx = len(warmup_output)\n",
    "    elif action_table_length >= sequence_length:\n",
    "        current_sequence = last_20_values.reshape(1, sequence_length)\n",
    "        action_table[:sequence_length] = last_20_values\n",
    "        start_idx = sequence_length\n",
    "    else:  # In case action_table_length is less than sequence_length\n",
    "        assign_length = min(sequence_length, action_table_length)\n",
    "        action_table[:assign_length] = last_20_values[-assign_length:]\n",
    "        current_sequence = last_20_values[-sequence_length:].reshape(1, sequence_length)\n",
    "        start_idx = assign_length\n",
    "\n",
    "    # Get the encoded label for the action\n",
    "    encoded_label = get_encoded_label_for_action(action)\n",
    "    encoded_label_array = np.array([encoded_label]).reshape(1, -1)\n",
    "    \n",
    "    for i in range(start_idx, action_table_length):\n",
    "        # Predict the next value using the LSTM model\n",
    "        next_value = model.predict([current_sequence, encoded_label_array])\n",
    "        action_table[i] = next_value[0][0]\n",
    "        current_sequence = np.append(current_sequence[0, 1:], next_value[0]).reshape(1, sequence_length)\n",
    "\n",
    "    return action_table, action_table[-sequence_length:]\n",
    "\n",
    "\n",
    "def get_encoded_label_for_action(action):\n",
    "    return label_to_int[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e121d274-47fa-4a39-83a7-9c012671a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(time_in_minutes, transitions, stats, model, sequence_length, last_20_values):\n",
    "    # 50 samples/second * 60 seconds/minute = 3000 samples/minute\n",
    "    total_samples_needed = time_in_minutes * 3000\n",
    "\n",
    "    # Estimate average action length using stats\n",
    "    avg_action_length = np.mean([mean for mean, std in stats.values()])\n",
    "\n",
    "    # Estimate the number of actions needed\n",
    "    num_actions = int(total_samples_needed / avg_action_length)\n",
    "\n",
    "    # Generate action list\n",
    "    action_list = generate_actions(transitions, num_actions)\n",
    "    print(\"Number of action to generate \"+str(len(action_list)))\n",
    "    # Create a dataframe to store the results\n",
    "    df = pd.DataFrame(columns=['label', 'norm'])\n",
    "    \n",
    "    for index, action in enumerate(action_list):\n",
    "        print(\"Generating action number: \"+str(index))\n",
    "        action_table_length = generate_length_for_action(action, stats)\n",
    "        \n",
    "        # Ensure the first action has a minimum length of 20\n",
    "        if index == 0 and action_table_length < 20:\n",
    "            action_table_length = 20\n",
    "\n",
    "        filled_table, new_last_values = fill_table_with_sequence(model, action_table_length, sequence_length, action, last_20_values)\n",
    "        \n",
    "        # Update last_20_values\n",
    "        if len(filled_table) < sequence_length:\n",
    "            last_20_values = np.concatenate((last_20_values[len(filled_table):], filled_table))\n",
    "        else:\n",
    "            last_20_values = new_last_values\n",
    "        \n",
    "        # Append to dataframe\n",
    "        df.loc[len(df)] = [action, filled_table.tolist()]  # store the filled_table as a list\n",
    "\n",
    "    # Return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeca1a12-2972-4cf0-8aee-ad75fc7dc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_match_number(directory_path):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "    \n",
    "    # Regular expression pattern to match our file names and extract the number\n",
    "    pattern = re.compile(r'output_Match_(\\d+)')\n",
    "    \n",
    "    # Extract all match numbers from filenames\n",
    "    match_numbers = [int(match.group(1)) for file in files for match in [pattern.search(file)] if match]\n",
    "    \n",
    "    # Return the next match number\n",
    "    if match_numbers:\n",
    "        return max(match_numbers) + 1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37f1217-b76c-4848-b339-180a877d3cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] time_in_minutes\n",
      "ipykernel_launcher.py: error: argument time_in_minutes: invalid int value: 'C:\\\\Users\\\\rabah\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-29c2d21e-68e6-4209-9f54-952dbf77647d.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2529\u001b[0m, in \u001b[0;36mArgumentParser._get_value\u001b[1;34m(self, action, arg_string)\u001b[0m\n\u001b[0;32m   2528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2529\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtype_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;66;03m# ArgumentTypeErrors indicate errors\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'C:\\\\Users\\\\rabah\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-29c2d21e-68e6-4209-9f54-952dbf77647d.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:1902\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2117\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n\u001b[1;32m-> 2117\u001b[0m stop_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_positionals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;66;03m# if we didn't consume all the argument strings, there were extras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2073\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_positionals\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   2072\u001b[0m     start_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m arg_count\n\u001b[1;32m-> 2073\u001b[0m     \u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;66;03m# slice off the Positionals that we just parsed and return the\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;66;03m# index at which the Positionals' string args stopped\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:1962\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.take_action\u001b[1;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[0;32m   1961\u001b[0m seen_actions\u001b[38;5;241m.\u001b[39madd(action)\n\u001b[1;32m-> 1962\u001b[0m argument_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument_strings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;66;03m# error if this argument is not allowed with other previously\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;66;03m# seen arguments, assuming that actions that use the default\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m \u001b[38;5;66;03m# value don't really count as \"present\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2496\u001b[0m, in \u001b[0;36mArgumentParser._get_values\u001b[1;34m(self, action, arg_strings)\u001b[0m\n\u001b[0;32m   2495\u001b[0m arg_string, \u001b[38;5;241m=\u001b[39m arg_strings\n\u001b[1;32m-> 2496\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_value(action, value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2542\u001b[0m, in \u001b[0;36mArgumentParser._get_value\u001b[1;34m(self, action, arg_string)\u001b[0m\n\u001b[0;32m   2541\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid \u001b[39m\u001b[38;5;132;01m%(type)s\u001b[39;00m\u001b[38;5;124m value: \u001b[39m\u001b[38;5;132;01m%(value)r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m args)\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# return the converted value\u001b[39;00m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument time_in_minutes: invalid int value: 'C:\\\\Users\\\\rabah\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-29c2d21e-68e6-4209-9f54-952dbf77647d.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m\n\u001b[0;32m     33\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_in_minutes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration for which data should be generated (in minutes).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m main(args\u001b[38;5;241m.\u001b[39mtime_in_minutes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:1869\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1869\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:1904\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1903\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1904\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2097\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2095\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2096\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2097\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2098\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2101\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def main(time_in_minutes):\n",
    "    last_20_values = np.zeros(20)\n",
    "    model_path = \"model/lstm_norm_values.keras\"\n",
    "    model = load_model(model_path)\n",
    "    df_result = generateData(time_in_minutes, transitions, stats, model, sequence_length, last_20_values)\n",
    "\n",
    "    df = df_result\n",
    "    # mean and std are previously defined\n",
    "    for idx, row in df.iterrows():\n",
    "        norm_array = np.array(row['norm'])\n",
    "        reversed_norm = (norm_array * std) + mean\n",
    "        df.at[idx, 'norm'] = reversed_norm.tolist()\n",
    "\n",
    "\n",
    "    def get_next_match_number(directory_path):\n",
    "        files = os.listdir(directory_path)\n",
    "        pattern = re.compile(r'output_Match_(\\d+)')\n",
    "        match_numbers = [int(match.group(1)) for file in files for match in [pattern.search(file)] if match]\n",
    "        if match_numbers:\n",
    "            return max(match_numbers) + 1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the dataframe to a list of dictionaries\n",
    "    formatted_data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Save to JSON file\n",
    "    directory_path = \"generatedData/\"\n",
    "    next_match_number = get_next_match_number(directory_path)\n",
    "    filename = f\"generatedData/output_Match_{next_match_number}_{time_in_minutes}_Min.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(formatted_data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Generate data for a specified time duration.')\n",
    "    parser.add_argument('time_in_minutes', type=int, help='Duration for which data should be generated (in minutes).')\n",
    "    args = parser.parse_args()\n",
    "    main(args.time_in_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cc3fd-f4f8-4cf3-85e7-26a1642e5054",
   "metadata": {},
   "source": [
    "If you want to generate from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ffc7240-4c71-4fb9-a1f6-ef31b13f6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_in_minutes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3137e48-a95f-409f-a3d5-8f5f7d25172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "# last_20_values = np.zeros(20)\n",
    "\n",
    "\n",
    "\n",
    "# model_path = \"model/lstm_norm_values.keras\"\n",
    "# model = load_model(model_path)\n",
    "\n",
    "# df_result = generateData(time_in_minutes, transitions, stats, model, sequence_length, last_20_values)\n",
    "# print(df_result)\n",
    "\n",
    "# df=df_result\n",
    "# # mean and std are previously defined\n",
    "# for idx, row in df.iterrows():\n",
    "#     # Convert the list to numpy array\n",
    "#     norm_array = np.array(row['norm'])\n",
    "    \n",
    "#     # Reverse the standardization\n",
    "#     reversed_norm = (norm_array * std) + mean\n",
    "    \n",
    "#     # Update the 'norm' column in the dataframe\n",
    "#     df.at[idx, 'norm'] = reversed_norm.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Display the updated dataframe\n",
    "# print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Convert the dataframe to a list of dictionaries\n",
    "# formatted_data = df.to_dict(orient='records')\n",
    "\n",
    "# # Save to JSON file\n",
    "# directory_path = \"generatedData/\"\n",
    "# next_match_number = get_next_match_number(directory_path)\n",
    "# filename = f\"generatedData/output_Match_{next_match_number}_{time_in_minutes}_Min.json\"\n",
    "# with open(filename, 'w') as f:\n",
    "#     json.dump(formatted_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150757c-122a-40db-b099-a8dd23f0129b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
